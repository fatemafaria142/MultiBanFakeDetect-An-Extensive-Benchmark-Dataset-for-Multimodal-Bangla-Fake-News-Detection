{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-07-17T12:57:00.215195Z","iopub.status.busy":"2024-07-17T12:57:00.214433Z","iopub.status.idle":"2024-07-17T12:57:03.148136Z","shell.execute_reply":"2024-07-17T12:57:03.147189Z","shell.execute_reply.started":"2024-07-17T12:57:00.215159Z"},"trusted":true},"outputs":[],"source":["import pandas as pd\n","import os\n","from sklearn.preprocessing import LabelEncoder\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision import transforms\n","from PIL import Image\n","from transformers import BertTokenizer, BertModel, AdamW\n","from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix\n","import torchvision.models as models\n","from torch import nn\n","import time \n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","from tqdm import tqdm\n","\n","# Paths to the CSV files and image directories\n","csv_paths = {\n","    'train': '/kaggle/input/fake-news/Train.csv',\n","    'test': '/kaggle/input/fake-news/Test.csv',\n","    'validation': '/kaggle/input/fake-news/Val.csv'\n","}\n","\n","image_dirs = {\n","    'train': '/kaggle/input/fake-news/train',\n","    'test': '/kaggle/input/fake-news/test',\n","    'validation': '/kaggle/input/fake-news/validation'\n","}\n","output_dir = '/kaggle/working/'  # Output directory to save the CSV files\n","\n","# Function to check for matching Meme_ID and image files, and add image paths\n","def check_matches(csv_path, image_dir):\n","    # Read the CSV file into a DataFrame\n","    df = pd.read_csv(csv_path)\n","    \n","    # List all files in the image directory\n","    image_files = os.listdir(image_dir)\n","    \n","    # Create a dictionary to map image filenames (without extensions) to their full paths\n","    image_names = {os.path.splitext(image_file)[0]: os.path.join(image_dir, image_file) for image_file in image_files}\n","    \n","    # Add an Image_Path column to the DataFrame\n","    df['Image_Path'] = df['image_id'].apply(lambda x: image_names.get(x, None))\n","    \n","    return df\n","\n","# Function to encode Intent_Taxonomy classes into labels\n","def encode_labels(df):\n","    label_encoder = LabelEncoder()\n","    df['label'] = label_encoder.fit_transform(df['label'])\n","    return df, label_encoder.classes_\n","\n","# Check matches for each set (Train, Test, Validation)\n","for key in csv_paths:\n","    matched_df = check_matches(csv_paths[key], image_dirs[key])\n","    \n","    matches_output_path = os.path.join(output_dir, f'{key}_matches.csv')\n","    \n","    # Save the processed dataframe to CSV\n","    matched_df.to_csv(matches_output_path, index=False)\n","    \n","    print(f\"{key} set:\")\n","    print(f\"Matched image_ids with image paths and labels saved to {matches_output_path}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-17T12:57:03.150682Z","iopub.status.busy":"2024-07-17T12:57:03.150047Z","iopub.status.idle":"2024-07-17T12:57:03.423590Z","shell.execute_reply":"2024-07-17T12:57:03.422678Z","shell.execute_reply.started":"2024-07-17T12:57:03.150647Z"},"trusted":true},"outputs":[],"source":["train_df = pd.read_csv('/kaggle/working/train_matches.csv')\n","train_df"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-17T12:57:03.425149Z","iopub.status.busy":"2024-07-17T12:57:03.424777Z","iopub.status.idle":"2024-07-17T12:57:03.475141Z","shell.execute_reply":"2024-07-17T12:57:03.474274Z","shell.execute_reply.started":"2024-07-17T12:57:03.425094Z"},"trusted":true},"outputs":[],"source":["test_df = pd.read_csv('/kaggle/working/test_matches.csv')\n","test_df"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-17T12:57:03.477691Z","iopub.status.busy":"2024-07-17T12:57:03.477306Z","iopub.status.idle":"2024-07-17T12:57:03.532552Z","shell.execute_reply":"2024-07-17T12:57:03.531691Z","shell.execute_reply.started":"2024-07-17T12:57:03.477655Z"},"trusted":true},"outputs":[],"source":["validation_df = pd.read_csv('/kaggle/working/validation_matches.csv')\n","validation_df"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-17T12:57:03.534113Z","iopub.status.busy":"2024-07-17T12:57:03.533835Z","iopub.status.idle":"2024-07-17T12:57:03.543213Z","shell.execute_reply":"2024-07-17T12:57:03.542421Z","shell.execute_reply.started":"2024-07-17T12:57:03.534091Z"},"trusted":true},"outputs":[],"source":["# Define your transformations using transforms.Compose\n","transform = transforms.Compose([\n","    transforms.Resize(224),\n","    transforms.CenterCrop(224),  # Crop the center to 224x224\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n","])\n","\n","class MyMultimodalDataset(Dataset):\n","    def __init__(self, image_paths, description, label, transform=None):\n","        self.image_paths = image_paths\n","        self.description = description\n","        self.label = label\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.image_paths)\n","\n","    def __getitem__(self, idx):\n","        img_path = self.image_paths[idx]\n","        text = self.description[idx]\n","        label = self.label[idx]\n","        #print(img_path)\n","        \n","        # Ensure img_path is a string\n","        if not isinstance(img_path, str):\n","            raise ValueError(f\"Invalid image path at index {idx}: {img_path}\")\n","        # Load and preprocess image\n","        image = Image.open(img_path).convert('RGB')\n","        if self.transform:\n","            image = self.transform(image)\n","        \n","        return image, text, label"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-17T12:57:03.544749Z","iopub.status.busy":"2024-07-17T12:57:03.544427Z","iopub.status.idle":"2024-07-17T12:57:03.555466Z","shell.execute_reply":"2024-07-17T12:57:03.554641Z","shell.execute_reply.started":"2024-07-17T12:57:03.544721Z"},"trusted":true},"outputs":[],"source":["print(train_df.columns)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-17T12:57:03.556912Z","iopub.status.busy":"2024-07-17T12:57:03.556581Z","iopub.status.idle":"2024-07-17T12:57:03.564684Z","shell.execute_reply":"2024-07-17T12:57:03.563893Z","shell.execute_reply.started":"2024-07-17T12:57:03.556890Z"},"trusted":true},"outputs":[],"source":["# Assuming you have lists or arrays of image paths, captions, and encoded labels:\n","train_dataset = MyMultimodalDataset(train_df['Image_Path'], train_df['description'], train_df['label'], transform=transform)\n","val_dataset = MyMultimodalDataset(validation_df['Image_Path'],validation_df['description'], validation_df['label'], transform=transform)\n","test_dataset = MyMultimodalDataset(test_df['Image_Path'],test_df['description'], test_df['label'], transform=transform)\n","\n","# Define data loaders\n","train_loader = DataLoader(train_dataset, batch_size=10, shuffle=True)\n","val_loader = DataLoader(val_dataset, batch_size=10, shuffle=False)\n","test_loader = DataLoader(test_dataset, batch_size=10, shuffle=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-17T12:57:03.636702Z","iopub.status.busy":"2024-07-17T12:57:03.636398Z","iopub.status.idle":"2024-07-17T12:57:03.647555Z","shell.execute_reply":"2024-07-17T12:57:03.646515Z","shell.execute_reply.started":"2024-07-17T12:57:03.636679Z"},"trusted":true},"outputs":[],"source":["train_df['label'].value_counts()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-17T12:57:03.895006Z","iopub.status.busy":"2024-07-17T12:57:03.894411Z","iopub.status.idle":"2024-07-17T12:57:03.902038Z","shell.execute_reply":"2024-07-17T12:57:03.901016Z","shell.execute_reply.started":"2024-07-17T12:57:03.894979Z"},"trusted":true},"outputs":[],"source":["validation_df['label'].value_counts()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-17T12:57:04.114798Z","iopub.status.busy":"2024-07-17T12:57:04.114066Z","iopub.status.idle":"2024-07-17T12:57:04.122197Z","shell.execute_reply":"2024-07-17T12:57:04.121297Z","shell.execute_reply.started":"2024-07-17T12:57:04.114768Z"},"trusted":true},"outputs":[],"source":["test_df['label'].value_counts()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-17T12:57:04.301378Z","iopub.status.busy":"2024-07-17T12:57:04.301047Z","iopub.status.idle":"2024-07-17T12:57:04.432153Z","shell.execute_reply":"2024-07-17T12:57:04.431196Z","shell.execute_reply.started":"2024-07-17T12:57:04.301353Z"},"trusted":true},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torchvision.models import mobilenet_v2\n","from transformers import BertModel, BertTokenizer\n","from tqdm import tqdm\n","import torchvision.models as models\n","import time\n","\n","# Initialize mobilenet_v2 with IMAGENET1K_V1 weights\n","mobilenet_v2 = models.mobilenet_v2(weights='IMAGENET1K_V1', progress=True)\n","mobilenet_v2 = torch.nn.Sequential(*(list(mobilenet_v2.children())[:-1]))  # Remove the classification laye # Remove the classification layer"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-17T12:58:35.912093Z","iopub.status.busy":"2024-07-17T12:58:35.911660Z","iopub.status.idle":"2024-07-17T12:58:50.651460Z","shell.execute_reply":"2024-07-17T12:58:50.650602Z","shell.execute_reply.started":"2024-07-17T12:58:35.912061Z"},"trusted":true},"outputs":[],"source":["# from transformers import BertTokenizer, BertModel,AdamW\n","# # Initialize BERT tokenizer and model\n","# bert_tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n","# bert_model = BertModel.from_pretrained(\"bert-base-multilingual-cased\")\n","\n","from transformers import AutoTokenizer, XLMRobertaModel, AdamW\n","# Initialize BERT tokenizer and model\n","bert_tokenizer = AutoTokenizer.from_pretrained(\"xlm-roberta-base\")\n","bert_model = XLMRobertaModel.from_pretrained(\"xlm-roberta-base\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-17T12:57:09.738801Z","iopub.status.busy":"2024-07-17T12:57:09.738464Z","iopub.status.idle":"2024-07-17T12:57:09.799502Z","shell.execute_reply":"2024-07-17T12:57:09.798303Z","shell.execute_reply.started":"2024-07-17T12:57:09.738773Z"},"trusted":true},"outputs":[],"source":["# Check if GPU is available\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print('Using device:', device)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-17T12:57:09.801312Z","iopub.status.busy":"2024-07-17T12:57:09.800970Z","iopub.status.idle":"2024-07-17T12:57:10.041322Z","shell.execute_reply":"2024-07-17T12:57:10.040405Z","shell.execute_reply.started":"2024-07-17T12:57:09.801267Z"},"trusted":true},"outputs":[],"source":["mobilenet_v2.to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-17T12:57:10.044081Z","iopub.status.busy":"2024-07-17T12:57:10.043467Z","iopub.status.idle":"2024-07-17T12:57:10.246607Z","shell.execute_reply":"2024-07-17T12:57:10.245504Z","shell.execute_reply.started":"2024-07-17T12:57:10.044049Z"},"trusted":true},"outputs":[],"source":["bert_model.to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-17T12:57:10.892838Z","iopub.status.busy":"2024-07-17T12:57:10.892465Z","iopub.status.idle":"2024-07-17T12:57:10.897978Z","shell.execute_reply":"2024-07-17T12:57:10.896934Z","shell.execute_reply.started":"2024-07-17T12:57:10.892809Z"},"trusted":true},"outputs":[],"source":["import torch\n","import time\n","from torch.optim import AdamW\n","from torchvision import transforms\n","from PIL import Image\n","from tqdm import tqdm"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-17T12:57:11.851719Z","iopub.status.busy":"2024-07-17T12:57:11.851319Z","iopub.status.idle":"2024-07-17T12:57:11.859254Z","shell.execute_reply":"2024-07-17T12:57:11.858292Z","shell.execute_reply.started":"2024-07-17T12:57:11.851679Z"},"trusted":true},"outputs":[],"source":["# Define optimizer and loss function\n","optimizer = AdamW(list(mobilenet_v2.parameters()) + list(bert_model.parameters()), lr=2e-5)\n","criterion = torch.nn.CrossEntropyLoss()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-17T12:57:48.701657Z","iopub.status.busy":"2024-07-17T12:57:48.701112Z","iopub.status.idle":"2024-07-17T12:58:07.351637Z","shell.execute_reply":"2024-07-17T12:58:07.350193Z","shell.execute_reply.started":"2024-07-17T12:57:48.701626Z"},"trusted":true},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from tqdm import tqdm\n","\n","class CombinedClassifier(nn.Module):\n","    def __init__(self, img_feature_dim, text_feature_dim, num_classes):\n","        super(CombinedClassifier, self).__init__()\n","        self.img_fc = nn.Sequential(\n","            nn.Linear(img_feature_dim, 512),\n","            nn.ReLU(),\n","            nn.Dropout(0.3)\n","        )\n","        self.text_fc = nn.Sequential(\n","            nn.Linear(text_feature_dim, 512),\n","            nn.ReLU(),\n","            nn.Dropout(0.5)\n","        )\n","        self.fusion_fc = nn.Sequential(\n","            nn.Linear(1024, 512),\n","            nn.ReLU(),\n","            nn.Dropout(0.3),\n","            nn.Linear(512, num_classes)\n","        )\n","\n","    def forward(self, img_features, text_features):\n","        img_feats = self.img_fc(img_features)\n","        text_feats = self.text_fc(text_features)\n","        fused_features = torch.cat((img_feats, text_feats), dim=-1)  # Concatenate along the feature dimension\n","        combined_logits = self.fusion_fc(fused_features)\n","        return combined_logits\n","\n","\n","# img_feature_dim\n","combined_classifier = CombinedClassifier(img_feature_dim=62720, text_feature_dim=768, num_classes=2).to(device)\n","\n","# Define optimizer and criterion\n","optimizer = optim.AdamW(list(mobilenet_v2.parameters()) + list(bert_model.parameters()) + list(combined_classifier.parameters()), lr=1e-5)\n","criterion = nn.CrossEntropyLoss()\n","\n","# Set number of epochs and other parameters\n","num_epochs = 35\n","max_seq_length = 80\n","\n","train_losses = []\n","train_accuracies = []\n","val_losses = []\n","val_accuracies = []\n","\n","start_time = time.time()\n","\n","# Training loop\n","for epoch in range(num_epochs):\n","    mobilenet_v2.train()\n","    bert_model.train()\n","    combined_classifier.train()\n","    \n","    running_train_loss = 0.0\n","    correct_train = 0\n","    total_train = 0\n","\n","    for images, texts, labels in tqdm(train_loader, desc=f'Epoch {epoch + 1}/{num_epochs}', leave=False):\n","        # Move tensors to the device\n","        images = images.to(device)\n","        labels = labels.to(device)\n","\n","        # Extract image features using MobileNetV2\n","        with torch.no_grad():\n","            img_feats = mobilenet_v2(images)\n","        \n","        # Reshape img_feats\n","        img_feats = img_feats.view(img_feats.size(0), -1)\n","\n","        # Convert texts to tensors and pad to a fixed sequence length\n","        texts = [bert_tokenizer(text, padding='max_length', truncation=True, max_length=max_seq_length, return_tensors='pt') for text in texts]\n","        input_ids = torch.stack([text['input_ids'].squeeze(0) for text in texts], dim=0).to(device)\n","        attention_mask = torch.stack([text['attention_mask'].squeeze(0) for text in texts], dim=0).to(device)\n","\n","        optimizer.zero_grad()\n","\n","        outputs = bert_model(input_ids, attention_mask=attention_mask)\n","        text_feats = outputs.last_hidden_state[:, 0, :]\n","\n","        combined_logits = combined_classifier(img_feats, text_feats)\n","\n","        # Ensure the labels tensor is flattened to match the combined_logits batch size\n","        labels = labels.view(-1)\n","\n","        loss = criterion(combined_logits, labels)\n","\n","        loss.backward()\n","        optimizer.step()\n","\n","        running_train_loss += loss.item()\n","        _, predicted = combined_logits.max(1)\n","        total_train += labels.size(0)\n","        correct_train += predicted.eq(labels).sum().item()\n","\n","    epoch_train_loss = running_train_loss / len(train_loader)\n","    epoch_train_accuracy = correct_train / total_train\n","\n","    train_losses.append(epoch_train_loss)\n","    train_accuracies.append(epoch_train_accuracy)\n","    \n","    # Validation loop\n","    mobilenet_v2.eval()\n","    bert_model.eval()\n","    combined_classifier.eval()\n","\n","    running_val_loss = 0.0\n","    correct_val = 0\n","    total_val = 0\n","\n","    with torch.no_grad():\n","        for val_images, val_texts, val_labels in val_loader:\n","            val_images = val_images.to(device)\n","            val_labels = val_labels.to(device)\n","\n","            val_img_feats = mobilenet_v2(val_images)\n","            val_img_feats = val_img_feats.view(val_img_feats.size(0), -1)\n","\n","            val_texts = [bert_tokenizer(text, padding='max_length', truncation=True, max_length=max_seq_length, return_tensors='pt') for text in val_texts]\n","            val_input_ids = torch.stack([text['input_ids'].squeeze(0) for text in val_texts], dim=0).to(device)\n","            val_attention_mask = torch.stack([text['attention_mask'].squeeze(0) for text in val_texts], dim=0).to(device)\n","\n","            val_outputs = bert_model(val_input_ids, attention_mask=val_attention_mask)\n","            val_text_feats = val_outputs.last_hidden_state[:, 0, :]\n","\n","            val_combined_logits = combined_classifier(val_img_feats, val_text_feats)\n","\n","            # Ensure the val_labels tensor is flattened to match the val_combined_logits batch size\n","            val_labels = val_labels.view(-1)\n","\n","            val_loss = criterion(val_combined_logits, val_labels)\n","\n","            running_val_loss += val_loss.item()\n","            _, val_predicted = val_combined_logits.max(1)\n","            total_val += val_labels.size(0)\n","            correct_val += val_predicted.eq(val_labels).sum().item()\n","\n","    epoch_val_loss = running_val_loss / len(val_loader)\n","    epoch_val_accuracy = correct_val / total_val\n","\n","    val_losses.append(epoch_val_loss)\n","    val_accuracies.append(epoch_val_accuracy)\n","\n","    print(f\"Epoch [{epoch + 1}/{num_epochs}] - \"\n","          f\"Train Loss: {epoch_train_loss:.4f}, Train Acc: {epoch_train_accuracy:.4f}, \"\n","          f\"Val Loss: {epoch_val_loss:.4f}, Val Acc: {epoch_val_accuracy:.4f}\")\n","\n","end_time = time.time()\n","execution_time = end_time - start_time\n","print(f\"Total execution time: {execution_time:.2f} seconds\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-17T12:54:15.474855Z","iopub.status.busy":"2024-07-17T12:54:15.474567Z","iopub.status.idle":"2024-07-17T12:54:39.176380Z","shell.execute_reply":"2024-07-17T12:54:39.175423Z","shell.execute_reply.started":"2024-07-17T12:54:15.474829Z"},"trusted":true},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix\n","\n","# Assuming test_loader contains your test data loader\n","\n","combined_classifier.eval()  # Set the model to evaluation mode\n","\n","test_losses = []\n","test_accuracies = []\n","predictions = []\n","true_labels = []\n","#test_loader \n","with torch.no_grad():\n","    for images, texts, labels in tqdm(test_loader, desc='Testing', leave=False):\n","        images = images.to(device)\n","        labels = labels.to(device)\n","\n","        texts = [bert_tokenizer(text, padding='max_length', truncation=True, max_length=max_seq_length, return_tensors='pt') for text in texts]\n","        input_ids = torch.stack([text['input_ids'].squeeze(0) for text in texts], dim=0).to(device)\n","        attention_mask = torch.stack([text['attention_mask'].squeeze(0) for text in texts], dim=0).to(device)\n","\n","        img_feats = mobilenet_v2(images)\n","        img_feats = img_feats.view(img_feats.size(0), -1)\n","\n","        outputs = bert_model(input_ids, attention_mask=attention_mask)\n","        text_feats = outputs.last_hidden_state[:, 0, :]\n","\n","        logits = combined_classifier(img_feats, text_feats)\n","\n","        # Calculate loss if needed\n","        test_loss = criterion(logits, labels)\n","        test_losses.append(test_loss.item())\n","\n","        # Calculate accuracy\n","        _, predicted = logits.max(1)\n","        predictions.extend(predicted.cpu().numpy())\n","        true_labels.extend(labels.cpu().numpy())\n","        correct = predicted.eq(labels).sum().item()\n","        total = labels.size(0)\n","        test_accuracy = correct / total\n","        test_accuracies.append(test_accuracy)\n","\n","# Calculate overall test metrics\n","average_test_loss = sum(test_losses) / len(test_losses)\n","test_accuracy = accuracy_score(true_labels, predictions)\n","precision, recall, f1_score, _ = precision_recall_fscore_support(true_labels, predictions, average='weighted')\n","conf_matrix = confusion_matrix(true_labels, predictions)\n","\n","# Print and plot results\n","print(f\"Test Loss: {average_test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n","print(f\"Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1_score:.4f}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-17T12:54:39.178029Z","iopub.status.busy":"2024-07-17T12:54:39.177701Z","iopub.status.idle":"2024-07-17T12:54:39.184454Z","shell.execute_reply":"2024-07-17T12:54:39.183546Z","shell.execute_reply.started":"2024-07-17T12:54:39.177973Z"},"trusted":true},"outputs":[],"source":["conf_matrix"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-17T12:54:39.186379Z","iopub.status.busy":"2024-07-17T12:54:39.185885Z","iopub.status.idle":"2024-07-17T12:54:39.427211Z","shell.execute_reply":"2024-07-17T12:54:39.426219Z","shell.execute_reply.started":"2024-07-17T12:54:39.186344Z"},"trusted":true},"outputs":[],"source":["# Plot confusion matrix\n","plt.figure(figsize=(8, 6))\n","# Class names according to the label encoding mapping\n","class_names = ['Real', 'Fake']\n","sns.heatmap(conf_matrix, annot=True, cmap='Blues', fmt='d',xticklabels=class_names, yticklabels=class_names)\n","plt.xlabel('Predicted labels')\n","plt.ylabel('True labels')\n","plt.title('Confusion Matrix')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":5344715,"sourceId":8881204,"sourceType":"datasetVersion"},{"datasetId":5316483,"sourceId":8973996,"sourceType":"datasetVersion"}],"dockerImageVersionId":30733,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
