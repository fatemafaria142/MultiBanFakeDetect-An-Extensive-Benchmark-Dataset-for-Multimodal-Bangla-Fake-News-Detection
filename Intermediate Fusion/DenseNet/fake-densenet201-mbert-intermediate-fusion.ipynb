{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-07-17T11:39:26.804802Z","iopub.status.busy":"2024-07-17T11:39:26.804409Z","iopub.status.idle":"2024-07-17T11:39:31.982054Z","shell.execute_reply":"2024-07-17T11:39:31.981062Z","shell.execute_reply.started":"2024-07-17T11:39:26.804764Z"},"trusted":true},"outputs":[],"source":["import pandas as pd\n","import os\n","from sklearn.preprocessing import LabelEncoder\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision import transforms\n","from PIL import Image\n","from transformers import BertTokenizer, BertModel, AdamW\n","from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix\n","import torchvision.models as models\n","from torch import nn\n","import time \n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","from tqdm import tqdm\n","\n","# Paths to the CSV files and image directories\n","csv_paths = {\n","    'train': '/kaggle/input/fake-news/Train.csv',\n","    'test': '/kaggle/input/fake-news/Test.csv',\n","    'validation': '/kaggle/input/fake-news/Val.csv'\n","}\n","\n","image_dirs = {\n","    'train': '/kaggle/input/fake-news/train',\n","    'test': '/kaggle/input/fake-news/test',\n","    'validation': '/kaggle/input/fake-news/validation'\n","}\n","output_dir = '/kaggle/working/'  # Output directory to save the CSV files\n","\n","# Function to check for matching Meme_ID and image files, and add image paths\n","def check_matches(csv_path, image_dir):\n","    # Read the CSV file into a DataFrame\n","    df = pd.read_csv(csv_path)\n","    \n","    # List all files in the image directory\n","    image_files = os.listdir(image_dir)\n","    \n","    # Create a dictionary to map image filenames (without extensions) to their full paths\n","    image_names = {os.path.splitext(image_file)[0]: os.path.join(image_dir, image_file) for image_file in image_files}\n","    \n","    # Add an Image_Path column to the DataFrame\n","    df['Image_Path'] = df['image_id'].apply(lambda x: image_names.get(x, None))\n","    \n","    return df\n","\n","# Function to encode Intent_Taxonomy classes into labels\n","def encode_labels(df):\n","    label_encoder = LabelEncoder()\n","    df['label'] = label_encoder.fit_transform(df['label'])\n","    return df, label_encoder.classes_\n","\n","# Check matches for each set (Train, Test, Validation)\n","for key in csv_paths:\n","    matched_df = check_matches(csv_paths[key], image_dirs[key])\n","    \n","    matches_output_path = os.path.join(output_dir, f'{key}_matches.csv')\n","    \n","    # Save the processed dataframe to CSV\n","    matched_df.to_csv(matches_output_path, index=False)\n","    \n","    print(f\"{key} set:\")\n","    print(f\"Matched image_ids with image paths and labels saved to {matches_output_path}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-17T11:39:31.987201Z","iopub.status.busy":"2024-07-17T11:39:31.986912Z","iopub.status.idle":"2024-07-17T11:39:32.242975Z","shell.execute_reply":"2024-07-17T11:39:32.242030Z","shell.execute_reply.started":"2024-07-17T11:39:31.987176Z"},"trusted":true},"outputs":[],"source":["train_df = pd.read_csv('/kaggle/working/train_matches.csv')\n","train_df"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-17T11:39:32.245022Z","iopub.status.busy":"2024-07-17T11:39:32.244501Z","iopub.status.idle":"2024-07-17T11:39:32.299589Z","shell.execute_reply":"2024-07-17T11:39:32.298671Z","shell.execute_reply.started":"2024-07-17T11:39:32.244965Z"},"trusted":true},"outputs":[],"source":["test_df = pd.read_csv('/kaggle/working/test_matches.csv')\n","test_df"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-17T11:39:32.303564Z","iopub.status.busy":"2024-07-17T11:39:32.302948Z","iopub.status.idle":"2024-07-17T11:39:32.357740Z","shell.execute_reply":"2024-07-17T11:39:32.356828Z","shell.execute_reply.started":"2024-07-17T11:39:32.303535Z"},"trusted":true},"outputs":[],"source":["validation_df = pd.read_csv('/kaggle/working/validation_matches.csv')\n","validation_df"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-17T11:39:32.359103Z","iopub.status.busy":"2024-07-17T11:39:32.358820Z","iopub.status.idle":"2024-07-17T11:39:32.368205Z","shell.execute_reply":"2024-07-17T11:39:32.367209Z","shell.execute_reply.started":"2024-07-17T11:39:32.359079Z"},"trusted":true},"outputs":[],"source":["# Define your transformations using transforms.Compose\n","transform = transforms.Compose([\n","    transforms.Resize(224),\n","    transforms.CenterCrop(224),  # Crop the center to 224x224\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n","])\n","\n","class MyMultimodalDataset(Dataset):\n","    def __init__(self, image_paths, description, label, transform=None):\n","        self.image_paths = image_paths\n","        self.description = description\n","        self.label = label\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.image_paths)\n","\n","    def __getitem__(self, idx):\n","        img_path = self.image_paths[idx]\n","        text = self.description[idx]\n","        label = self.label[idx]\n","        #print(img_path)\n","        \n","        # Ensure img_path is a string\n","        if not isinstance(img_path, str):\n","            raise ValueError(f\"Invalid image path at index {idx}: {img_path}\")\n","        # Load and preprocess image\n","        image = Image.open(img_path).convert('RGB')\n","        if self.transform:\n","            image = self.transform(image)\n","        \n","        return image, text, label"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-17T11:39:32.369706Z","iopub.status.busy":"2024-07-17T11:39:32.369439Z","iopub.status.idle":"2024-07-17T11:39:32.381915Z","shell.execute_reply":"2024-07-17T11:39:32.381144Z","shell.execute_reply.started":"2024-07-17T11:39:32.369679Z"},"trusted":true},"outputs":[],"source":["print(train_df.columns)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-17T11:39:32.383276Z","iopub.status.busy":"2024-07-17T11:39:32.382997Z","iopub.status.idle":"2024-07-17T11:39:32.392259Z","shell.execute_reply":"2024-07-17T11:39:32.391433Z","shell.execute_reply.started":"2024-07-17T11:39:32.383247Z"},"trusted":true},"outputs":[],"source":["# Assuming you have lists or arrays of image paths, captions, and encoded labels:\n","train_dataset = MyMultimodalDataset(train_df['Image_Path'], train_df['description'], train_df['label'], transform=transform)\n","val_dataset = MyMultimodalDataset(validation_df['Image_Path'],validation_df['description'], validation_df['label'], transform=transform)\n","test_dataset = MyMultimodalDataset(test_df['Image_Path'],test_df['description'], test_df['label'], transform=transform)\n","\n","# Define data loaders\n","train_loader = DataLoader(train_dataset, batch_size=10, shuffle=True)\n","val_loader = DataLoader(val_dataset, batch_size=10, shuffle=False)\n","test_loader = DataLoader(test_dataset, batch_size=10, shuffle=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-17T11:39:32.393538Z","iopub.status.busy":"2024-07-17T11:39:32.393271Z","iopub.status.idle":"2024-07-17T11:39:32.405225Z","shell.execute_reply":"2024-07-17T11:39:32.404123Z","shell.execute_reply.started":"2024-07-17T11:39:32.393516Z"},"trusted":true},"outputs":[],"source":["train_df['label'].value_counts()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-17T11:39:32.406768Z","iopub.status.busy":"2024-07-17T11:39:32.406469Z","iopub.status.idle":"2024-07-17T11:39:32.415966Z","shell.execute_reply":"2024-07-17T11:39:32.415130Z","shell.execute_reply.started":"2024-07-17T11:39:32.406733Z"},"trusted":true},"outputs":[],"source":["validation_df['label'].value_counts()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-17T11:39:32.417337Z","iopub.status.busy":"2024-07-17T11:39:32.417054Z","iopub.status.idle":"2024-07-17T11:39:32.426507Z","shell.execute_reply":"2024-07-17T11:39:32.425727Z","shell.execute_reply.started":"2024-07-17T11:39:32.417313Z"},"trusted":true},"outputs":[],"source":["test_df['label'].value_counts()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-17T11:39:32.427887Z","iopub.status.busy":"2024-07-17T11:39:32.427592Z","iopub.status.idle":"2024-07-17T11:39:33.019429Z","shell.execute_reply":"2024-07-17T11:39:33.018403Z","shell.execute_reply.started":"2024-07-17T11:39:32.427862Z"},"trusted":true},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torchvision.models import densenet121, densenet169,densenet201\n","from transformers import BertModel, BertTokenizer\n","from tqdm import tqdm\n","import torchvision.models as models\n","import time\n","\n","# Initialize densenet201 with IMAGENET1K_V1 weights\n","densenet201 = models.densenet201(weights='IMAGENET1K_V1', progress=True)\n","densenet201 = torch.nn.Sequential(*(list(densenet201.children())[:-1]))  # Remove the classification laye # Remove the classification layer"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-17T11:39:33.021419Z","iopub.status.busy":"2024-07-17T11:39:33.020873Z","iopub.status.idle":"2024-07-17T11:39:33.836125Z","shell.execute_reply":"2024-07-17T11:39:33.835208Z","shell.execute_reply.started":"2024-07-17T11:39:33.021391Z"},"trusted":true},"outputs":[],"source":["from transformers import BertTokenizer, BertModel,AdamW\n","# Initialize BERT tokenizer and model\n","bert_tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n","bert_model = BertModel.from_pretrained(\"bert-base-multilingual-cased\")\n","\n","# from transformers import AutoTokenizer, XLMRobertaModel, AdamW\n","# # Initialize BERT tokenizer and model\n","# bert_tokenizer = AutoTokenizer.from_pretrained(\"xlm-roberta-base\")\n","# bert_model = XLMRobertaModel.from_pretrained(\"xlm-roberta-base\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-17T11:39:33.840635Z","iopub.status.busy":"2024-07-17T11:39:33.840333Z","iopub.status.idle":"2024-07-17T11:39:33.904870Z","shell.execute_reply":"2024-07-17T11:39:33.903808Z","shell.execute_reply.started":"2024-07-17T11:39:33.840610Z"},"trusted":true},"outputs":[],"source":["# Check if GPU is available\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print('Using device:', device)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-17T11:39:33.906785Z","iopub.status.busy":"2024-07-17T11:39:33.906333Z","iopub.status.idle":"2024-07-17T11:39:34.126122Z","shell.execute_reply":"2024-07-17T11:39:34.125163Z","shell.execute_reply.started":"2024-07-17T11:39:33.906751Z"},"trusted":true},"outputs":[],"source":["densenet201.to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-17T11:39:34.128298Z","iopub.status.busy":"2024-07-17T11:39:34.127494Z","iopub.status.idle":"2024-07-17T11:39:34.341523Z","shell.execute_reply":"2024-07-17T11:39:34.340652Z","shell.execute_reply.started":"2024-07-17T11:39:34.128261Z"},"trusted":true},"outputs":[],"source":["bert_model.to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-17T11:39:34.343218Z","iopub.status.busy":"2024-07-17T11:39:34.342796Z","iopub.status.idle":"2024-07-17T11:39:34.349078Z","shell.execute_reply":"2024-07-17T11:39:34.347999Z","shell.execute_reply.started":"2024-07-17T11:39:34.343180Z"},"trusted":true},"outputs":[],"source":["import torch\n","import time\n","from torch.optim import AdamW\n","from torchvision import transforms\n","from PIL import Image\n","from tqdm import tqdm"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-17T12:05:34.289555Z","iopub.status.busy":"2024-07-17T12:05:34.288843Z","iopub.status.idle":"2024-07-17T12:05:34.300932Z","shell.execute_reply":"2024-07-17T12:05:34.300042Z","shell.execute_reply.started":"2024-07-17T12:05:34.289521Z"},"trusted":true},"outputs":[],"source":["# Define optimizer and loss function\n","optimizer = AdamW(list(densenet201.parameters()) + list(bert_model.parameters()), lr=2e-5)\n","criterion = torch.nn.CrossEntropyLoss()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-17T12:05:35.209150Z","iopub.status.busy":"2024-07-17T12:05:35.208762Z","iopub.status.idle":"2024-07-17T12:11:11.687646Z","shell.execute_reply":"2024-07-17T12:11:11.686572Z","shell.execute_reply.started":"2024-07-17T12:05:35.209117Z"},"trusted":true},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from tqdm import tqdm\n","\n","class CombinedClassifier(nn.Module):\n","    def __init__(self, img_feature_dim, text_feature_dim, num_classes):\n","        super(CombinedClassifier, self).__init__()\n","        self.img_fc = nn.Sequential(\n","            nn.Linear(img_feature_dim, 512),\n","            nn.ReLU(),\n","            nn.Dropout(0.3)\n","        )\n","        self.text_fc = nn.Sequential(\n","            nn.Linear(text_feature_dim, 512),\n","            nn.ReLU(),\n","            nn.Dropout(0.5)\n","        )\n","        self.fusion_fc = nn.Sequential(\n","            nn.Linear(1024, 512),\n","            nn.ReLU(),\n","            nn.Dropout(0.3),\n","            nn.Linear(512, num_classes)\n","        )\n","\n","    def forward(self, img_features, text_features):\n","        img_feats = self.img_fc(img_features)\n","        text_feats = self.text_fc(text_features)\n","        fused_features = torch.cat((img_feats, text_feats), dim=-1)  # Concatenate along the feature dimension\n","        combined_logits = self.fusion_fc(fused_features)\n","        return combined_logits\n","\n","\n","# img_feature_dim\n","combined_classifier = CombinedClassifier(img_feature_dim=94080, text_feature_dim=768, num_classes=2).to(device)\n","\n","# Define optimizer and criterion\n","optimizer = optim.AdamW(list(densenet201.parameters()) + list(bert_model.parameters()) + list(combined_classifier.parameters()), lr=1e-5)\n","criterion = nn.CrossEntropyLoss()\n","\n","# Set number of epochs and other parameters\n","num_epochs = 35\n","max_seq_length = 80\n","\n","train_losses = []\n","train_accuracies = []\n","val_losses = []\n","val_accuracies = []\n","\n","start_time = time.time()\n","\n","# Training loop\n","for epoch in range(num_epochs):\n","    densenet201.train()\n","    bert_model.train()\n","    combined_classifier.train()\n","    \n","    running_train_loss = 0.0\n","    correct_train = 0\n","    total_train = 0\n","\n","    for images, texts, labels in tqdm(train_loader, desc=f'Epoch {epoch + 1}/{num_epochs}', leave=False):\n","        # Move tensors to the device\n","        images = images.to(device)\n","        labels = labels.to(device)\n","\n","        # Extract image features using MobileNetV2\n","        with torch.no_grad():\n","            img_feats = densenet201(images)\n","        \n","        # Reshape img_feats\n","        img_feats = img_feats.view(img_feats.size(0), -1)\n","\n","        # Convert texts to tensors and pad to a fixed sequence length\n","        texts = [bert_tokenizer(text, padding='max_length', truncation=True, max_length=max_seq_length, return_tensors='pt') for text in texts]\n","        input_ids = torch.stack([text['input_ids'].squeeze(0) for text in texts], dim=0).to(device)\n","        attention_mask = torch.stack([text['attention_mask'].squeeze(0) for text in texts], dim=0).to(device)\n","\n","        optimizer.zero_grad()\n","\n","        outputs = bert_model(input_ids, attention_mask=attention_mask)\n","        text_feats = outputs.last_hidden_state[:, 0, :]\n","\n","        combined_logits = combined_classifier(img_feats, text_feats)\n","\n","        # Ensure the labels tensor is flattened to match the combined_logits batch size\n","        labels = labels.view(-1)\n","\n","        loss = criterion(combined_logits, labels)\n","\n","        loss.backward()\n","        optimizer.step()\n","\n","        running_train_loss += loss.item()\n","        _, predicted = combined_logits.max(1)\n","        total_train += labels.size(0)\n","        correct_train += predicted.eq(labels).sum().item()\n","\n","    epoch_train_loss = running_train_loss / len(train_loader)\n","    epoch_train_accuracy = correct_train / total_train\n","\n","    train_losses.append(epoch_train_loss)\n","    train_accuracies.append(epoch_train_accuracy)\n","    \n","    # Validation loop\n","    densenet201.eval()\n","    bert_model.eval()\n","    combined_classifier.eval()\n","\n","    running_val_loss = 0.0\n","    correct_val = 0\n","    total_val = 0\n","\n","    with torch.no_grad():\n","        for val_images, val_texts, val_labels in val_loader:\n","            val_images = val_images.to(device)\n","            val_labels = val_labels.to(device)\n","\n","            val_img_feats = densenet201(val_images)\n","            val_img_feats = val_img_feats.view(val_img_feats.size(0), -1)\n","\n","            val_texts = [bert_tokenizer(text, padding='max_length', truncation=True, max_length=max_seq_length, return_tensors='pt') for text in val_texts]\n","            val_input_ids = torch.stack([text['input_ids'].squeeze(0) for text in val_texts], dim=0).to(device)\n","            val_attention_mask = torch.stack([text['attention_mask'].squeeze(0) for text in val_texts], dim=0).to(device)\n","\n","            val_outputs = bert_model(val_input_ids, attention_mask=val_attention_mask)\n","            val_text_feats = val_outputs.last_hidden_state[:, 0, :]\n","\n","            val_combined_logits = combined_classifier(val_img_feats, val_text_feats)\n","\n","            # Ensure the val_labels tensor is flattened to match the val_combined_logits batch size\n","            val_labels = val_labels.view(-1)\n","\n","            val_loss = criterion(val_combined_logits, val_labels)\n","\n","            running_val_loss += val_loss.item()\n","            _, val_predicted = val_combined_logits.max(1)\n","            total_val += val_labels.size(0)\n","            correct_val += val_predicted.eq(val_labels).sum().item()\n","\n","    epoch_val_loss = running_val_loss / len(val_loader)\n","    epoch_val_accuracy = correct_val / total_val\n","\n","    val_losses.append(epoch_val_loss)\n","    val_accuracies.append(epoch_val_accuracy)\n","\n","    print(f\"Epoch [{epoch + 1}/{num_epochs}] - \"\n","          f\"Train Loss: {epoch_train_loss:.4f}, Train Acc: {epoch_train_accuracy:.4f}, \"\n","          f\"Val Loss: {epoch_val_loss:.4f}, Val Acc: {epoch_val_accuracy:.4f}\")\n","\n","end_time = time.time()\n","execution_time = end_time - start_time\n","print(f\"Total execution time: {execution_time:.2f} seconds\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-17T12:11:11.689953Z","iopub.status.busy":"2024-07-17T12:11:11.689565Z","iopub.status.idle":"2024-07-17T12:11:36.083974Z","shell.execute_reply":"2024-07-17T12:11:36.082960Z","shell.execute_reply.started":"2024-07-17T12:11:11.689922Z"},"trusted":true},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix\n","\n","# Assuming test_loader contains your test data loader\n","\n","combined_classifier.eval()  # Set the model to evaluation mode\n","\n","test_losses = []\n","test_accuracies = []\n","predictions = []\n","true_labels = []\n","#test_loader \n","with torch.no_grad():\n","    for images, texts, labels in tqdm(test_loader, desc='Testing', leave=False):\n","        images = images.to(device)\n","        labels = labels.to(device)\n","\n","        texts = [bert_tokenizer(text, padding='max_length', truncation=True, max_length=max_seq_length, return_tensors='pt') for text in texts]\n","        input_ids = torch.stack([text['input_ids'].squeeze(0) for text in texts], dim=0).to(device)\n","        attention_mask = torch.stack([text['attention_mask'].squeeze(0) for text in texts], dim=0).to(device)\n","\n","        img_feats = densenet201(images)\n","        img_feats = img_feats.view(img_feats.size(0), -1)\n","\n","        outputs = bert_model(input_ids, attention_mask=attention_mask)\n","        text_feats = outputs.last_hidden_state[:, 0, :]\n","\n","        logits = combined_classifier(img_feats, text_feats)\n","\n","        # Calculate loss if needed\n","        test_loss = criterion(logits, labels)\n","        test_losses.append(test_loss.item())\n","\n","        # Calculate accuracy\n","        _, predicted = logits.max(1)\n","        predictions.extend(predicted.cpu().numpy())\n","        true_labels.extend(labels.cpu().numpy())\n","        correct = predicted.eq(labels).sum().item()\n","        total = labels.size(0)\n","        test_accuracy = correct / total\n","        test_accuracies.append(test_accuracy)\n","\n","# Calculate overall test metrics\n","average_test_loss = sum(test_losses) / len(test_losses)\n","test_accuracy = accuracy_score(true_labels, predictions)\n","precision, recall, f1_score, _ = precision_recall_fscore_support(true_labels, predictions, average='weighted')\n","conf_matrix = confusion_matrix(true_labels, predictions)\n","\n","# Print and plot results\n","print(f\"Test Loss: {average_test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n","print(f\"Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1_score:.4f}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-17T12:11:36.085439Z","iopub.status.busy":"2024-07-17T12:11:36.085144Z","iopub.status.idle":"2024-07-17T12:11:36.092037Z","shell.execute_reply":"2024-07-17T12:11:36.091157Z","shell.execute_reply.started":"2024-07-17T12:11:36.085412Z"},"trusted":true},"outputs":[],"source":["conf_matrix"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-17T12:11:36.094737Z","iopub.status.busy":"2024-07-17T12:11:36.094376Z","iopub.status.idle":"2024-07-17T12:11:36.316164Z","shell.execute_reply":"2024-07-17T12:11:36.315210Z","shell.execute_reply.started":"2024-07-17T12:11:36.094706Z"},"trusted":true},"outputs":[],"source":["# Plot confusion matrix\n","plt.figure(figsize=(8, 6))\n","# Class names according to the label encoding mapping\n","class_names = ['Real', 'Fake']\n","sns.heatmap(conf_matrix, annot=True, cmap='Blues', fmt='d',xticklabels=class_names, yticklabels=class_names)\n","plt.xlabel('Predicted labels')\n","plt.ylabel('True labels')\n","plt.title('Confusion Matrix')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":5344715,"sourceId":8881204,"sourceType":"datasetVersion"},{"datasetId":5316483,"sourceId":8973996,"sourceType":"datasetVersion"}],"dockerImageVersionId":30733,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
