{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-07-18T07:04:31.775868Z","iopub.status.busy":"2024-07-18T07:04:31.775468Z","iopub.status.idle":"2024-07-18T07:04:36.897650Z","shell.execute_reply":"2024-07-18T07:04:36.896534Z","shell.execute_reply.started":"2024-07-18T07:04:31.775837Z"},"trusted":true},"outputs":[],"source":["import pandas as pd\n","import os\n","from sklearn.preprocessing import LabelEncoder\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision import transforms\n","from PIL import Image\n","from transformers import BertTokenizer, BertModel, AdamW\n","from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix\n","import torchvision.models as models\n","from torch import nn\n","import time \n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","from tqdm import tqdm\n","\n","# Paths to the CSV files and image directories\n","csv_paths = {\n","    'train': '/kaggle/input/fake-news/Train.csv',\n","    'test': '/kaggle/input/fake-news/Test.csv',\n","    'validation': '/kaggle/input/fake-news/Val.csv'\n","}\n","\n","image_dirs = {\n","    'train': '/kaggle/input/fake-news/train',\n","    'test': '/kaggle/input/fake-news/test',\n","    'validation': '/kaggle/input/fake-news/validation'\n","}\n","output_dir = '/kaggle/working/'  # Output directory to save the CSV files\n","\n","# Function to check for matching Meme_ID and image files, and add image paths\n","def check_matches(csv_path, image_dir):\n","    # Read the CSV file into a DataFrame\n","    df = pd.read_csv(csv_path)\n","    \n","    # List all files in the image directory\n","    image_files = os.listdir(image_dir)\n","    \n","    # Create a dictionary to map image filenames (without extensions) to their full paths\n","    image_names = {os.path.splitext(image_file)[0]: os.path.join(image_dir, image_file) for image_file in image_files}\n","    \n","    # Add an Image_Path column to the DataFrame\n","    df['Image_Path'] = df['image_id'].apply(lambda x: image_names.get(x, None))\n","    \n","    return df\n","\n","# Function to encode Intent_Taxonomy classes into labels\n","def encode_labels(df):\n","    label_encoder = LabelEncoder()\n","    df['label'] = label_encoder.fit_transform(df['label'])\n","    return df, label_encoder.classes_\n","\n","# Check matches for each set (Train, Test, Validation)\n","for key in csv_paths:\n","    matched_df = check_matches(csv_paths[key], image_dirs[key])\n","    \n","    matches_output_path = os.path.join(output_dir, f'{key}_matches.csv')\n","    \n","    # Save the processed dataframe to CSV\n","    matched_df.to_csv(matches_output_path, index=False)\n","    \n","    print(f\"{key} set:\")\n","    print(f\"Matched image_ids with image paths and labels saved to {matches_output_path}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-18T07:04:36.900656Z","iopub.status.busy":"2024-07-18T07:04:36.900206Z","iopub.status.idle":"2024-07-18T07:04:37.193964Z","shell.execute_reply":"2024-07-18T07:04:37.192789Z","shell.execute_reply.started":"2024-07-18T07:04:36.900615Z"},"trusted":true},"outputs":[],"source":["train_df = pd.read_csv('/kaggle/working/train_matches.csv')\n","train_df"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-18T07:04:37.195787Z","iopub.status.busy":"2024-07-18T07:04:37.195401Z","iopub.status.idle":"2024-07-18T07:04:37.266786Z","shell.execute_reply":"2024-07-18T07:04:37.265546Z","shell.execute_reply.started":"2024-07-18T07:04:37.195757Z"},"trusted":true},"outputs":[],"source":["test_df = pd.read_csv('/kaggle/working/test_matches.csv')\n","test_df"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-18T07:04:37.268531Z","iopub.status.busy":"2024-07-18T07:04:37.268115Z","iopub.status.idle":"2024-07-18T07:04:37.334770Z","shell.execute_reply":"2024-07-18T07:04:37.333548Z","shell.execute_reply.started":"2024-07-18T07:04:37.268493Z"},"trusted":true},"outputs":[],"source":["validation_df = pd.read_csv('/kaggle/working/validation_matches.csv')\n","validation_df"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-18T07:04:37.341098Z","iopub.status.busy":"2024-07-18T07:04:37.340689Z","iopub.status.idle":"2024-07-18T07:04:37.353407Z","shell.execute_reply":"2024-07-18T07:04:37.351914Z","shell.execute_reply.started":"2024-07-18T07:04:37.341060Z"},"trusted":true},"outputs":[],"source":["# Define your transformations using transforms.Compose\n","transform = transforms.Compose([\n","    transforms.Resize(224),\n","    transforms.CenterCrop(224),  # Crop the center to 224x224\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n","])\n","\n","class MyMultimodalDataset(Dataset):\n","    def __init__(self, image_paths, description, label, transform=None):\n","        self.image_paths = image_paths\n","        self.description = description\n","        self.label = label\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.image_paths)\n","\n","    def __getitem__(self, idx):\n","        img_path = self.image_paths[idx]\n","        text = self.description[idx]\n","        label = self.label[idx]\n","        #print(img_path)\n","        \n","        # Ensure img_path is a string\n","        if not isinstance(img_path, str):\n","            raise ValueError(f\"Invalid image path at index {idx}: {img_path}\")\n","        # Load and preprocess image\n","        image = Image.open(img_path).convert('RGB')\n","        if self.transform:\n","            image = self.transform(image)\n","        \n","        return image, text, label"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-18T07:04:37.355324Z","iopub.status.busy":"2024-07-18T07:04:37.355004Z","iopub.status.idle":"2024-07-18T07:04:37.367621Z","shell.execute_reply":"2024-07-18T07:04:37.366537Z","shell.execute_reply.started":"2024-07-18T07:04:37.355294Z"},"trusted":true},"outputs":[],"source":["print(train_df.columns)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-18T07:04:37.369274Z","iopub.status.busy":"2024-07-18T07:04:37.368969Z","iopub.status.idle":"2024-07-18T07:04:37.378462Z","shell.execute_reply":"2024-07-18T07:04:37.377417Z","shell.execute_reply.started":"2024-07-18T07:04:37.369245Z"},"trusted":true},"outputs":[],"source":["# Assuming you have lists or arrays of image paths, captions, and encoded labels:\n","train_dataset = MyMultimodalDataset(train_df['Image_Path'], train_df['description'], train_df['label'], transform=transform)\n","val_dataset = MyMultimodalDataset(validation_df['Image_Path'],validation_df['description'], validation_df['label'], transform=transform)\n","test_dataset = MyMultimodalDataset(test_df['Image_Path'],test_df['description'], test_df['label'], transform=transform)\n","\n","# Define data loaders\n","train_loader = DataLoader(train_dataset, batch_size=10, shuffle=True)\n","val_loader = DataLoader(val_dataset, batch_size=10, shuffle=False)\n","test_loader = DataLoader(test_dataset, batch_size=10, shuffle=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-18T07:04:37.380619Z","iopub.status.busy":"2024-07-18T07:04:37.380249Z","iopub.status.idle":"2024-07-18T07:04:37.396618Z","shell.execute_reply":"2024-07-18T07:04:37.395307Z","shell.execute_reply.started":"2024-07-18T07:04:37.380556Z"},"trusted":true},"outputs":[],"source":["train_df['label'].value_counts()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-18T07:04:37.399791Z","iopub.status.busy":"2024-07-18T07:04:37.398016Z","iopub.status.idle":"2024-07-18T07:04:37.409211Z","shell.execute_reply":"2024-07-18T07:04:37.408065Z","shell.execute_reply.started":"2024-07-18T07:04:37.399763Z"},"trusted":true},"outputs":[],"source":["validation_df['label'].value_counts()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-18T07:04:37.413170Z","iopub.status.busy":"2024-07-18T07:04:37.412361Z","iopub.status.idle":"2024-07-18T07:04:37.421682Z","shell.execute_reply":"2024-07-18T07:04:37.420438Z","shell.execute_reply.started":"2024-07-18T07:04:37.413137Z"},"trusted":true},"outputs":[],"source":["test_df['label'].value_counts()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-18T07:04:37.423907Z","iopub.status.busy":"2024-07-18T07:04:37.423433Z","iopub.status.idle":"2024-07-18T07:04:38.175795Z","shell.execute_reply":"2024-07-18T07:04:38.174815Z","shell.execute_reply.started":"2024-07-18T07:04:37.423876Z"},"trusted":true},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torchvision.models import densenet121, densenet169,densenet201\n","from transformers import BertModel, BertTokenizer\n","from tqdm import tqdm\n","import torchvision.models as models\n","import time\n","\n","# Initialize densenet121 with IMAGENET1K_V1 weights\n","densenet121 = models.densenet121(weights='IMAGENET1K_V1', progress=True)\n","densenet121 = torch.nn.Sequential(*(list(densenet121.children())[:-1]))  # Remove the classification laye # Remove the classification layer"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-18T07:21:13.693745Z","iopub.status.busy":"2024-07-18T07:21:13.693093Z","iopub.status.idle":"2024-07-18T07:21:22.479970Z","shell.execute_reply":"2024-07-18T07:21:22.478927Z","shell.execute_reply.started":"2024-07-18T07:21:13.693700Z"},"trusted":true},"outputs":[],"source":["# from transformers import BertTokenizer, BertModel,AdamW\n","# # Initialize BERT tokenizer and model\n","# bert_tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n","# bert_model = BertModel.from_pretrained(\"bert-base-multilingual-cased\")\n","\n","from transformers import AutoTokenizer, XLMRobertaModel, AdamW\n","# Initialize BERT tokenizer and model\n","bert_tokenizer = AutoTokenizer.from_pretrained(\"xlm-roberta-base\")\n","bert_model = XLMRobertaModel.from_pretrained(\"xlm-roberta-base\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-18T07:21:23.014827Z","iopub.status.busy":"2024-07-18T07:21:23.014008Z","iopub.status.idle":"2024-07-18T07:21:23.019912Z","shell.execute_reply":"2024-07-18T07:21:23.018980Z","shell.execute_reply.started":"2024-07-18T07:21:23.014794Z"},"trusted":true},"outputs":[],"source":["# Check if GPU is available\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print('Using device:', device)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-18T07:21:24.479058Z","iopub.status.busy":"2024-07-18T07:21:24.478339Z","iopub.status.idle":"2024-07-18T07:21:24.499522Z","shell.execute_reply":"2024-07-18T07:21:24.498555Z","shell.execute_reply.started":"2024-07-18T07:21:24.479023Z"},"trusted":true},"outputs":[],"source":["densenet121.to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-18T07:21:25.471906Z","iopub.status.busy":"2024-07-18T07:21:25.471438Z","iopub.status.idle":"2024-07-18T07:21:25.812294Z","shell.execute_reply":"2024-07-18T07:21:25.811268Z","shell.execute_reply.started":"2024-07-18T07:21:25.471859Z"},"trusted":true},"outputs":[],"source":["bert_model.to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-18T07:06:51.088371Z","iopub.status.busy":"2024-07-18T07:06:51.087569Z","iopub.status.idle":"2024-07-18T07:06:51.093380Z","shell.execute_reply":"2024-07-18T07:06:51.092183Z","shell.execute_reply.started":"2024-07-18T07:06:51.088338Z"},"trusted":true},"outputs":[],"source":["import torch\n","import time\n","from torch.optim import AdamW\n","from torchvision import transforms\n","from PIL import Image\n","from tqdm import tqdm"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-18T07:06:53.543191Z","iopub.status.busy":"2024-07-18T07:06:53.542805Z","iopub.status.idle":"2024-07-18T07:06:53.552078Z","shell.execute_reply":"2024-07-18T07:06:53.550991Z","shell.execute_reply.started":"2024-07-18T07:06:53.543160Z"},"trusted":true},"outputs":[],"source":["# Define optimizer and loss function\n","optimizer = AdamW(list(densenet121.parameters()) + list(bert_model.parameters()), lr=2e-5)\n","criterion = torch.nn.CrossEntropyLoss()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-18T07:07:40.987415Z","iopub.status.busy":"2024-07-18T07:07:40.986959Z","iopub.status.idle":"2024-07-18T07:14:22.492735Z","shell.execute_reply":"2024-07-18T07:14:22.491629Z","shell.execute_reply.started":"2024-07-18T07:07:40.987365Z"},"trusted":true},"outputs":[],"source":["class CombinedClassifier(nn.Module):\n","    def __init__(self, img_feature_dim, text_feature_dim, num_classes):\n","        super(CombinedClassifier, self).__init__()\n","        self.img_fc = nn.Sequential(\n","            nn.Linear(img_feature_dim, 512),\n","            nn.ReLU(),\n","            nn.Dropout(0.3),\n","            nn.Linear(512, num_classes)\n","        )\n","        self.text_fc = nn.Sequential(\n","            nn.Linear(text_feature_dim, 512),\n","            nn.ReLU(),\n","            nn.Dropout(0.5),\n","            nn.Linear(512, num_classes)\n","        )\n","\n","    def forward(self, img_features, text_features):\n","        img_logits = self.img_fc(img_features)\n","        text_logits = self.text_fc(text_features)\n","        combined_logits = 0.5 * (img_logits + text_logits)  # Simple averaging\n","        return combined_logits\n","\n","# Example usage assuming img_feature_dim is correctly set\n","# Update img_feature_dim based on the actual output dimension of MobileNetV3\n","img_feature_dim = 50176  # Example corrected dimension, update based on actual output\n","combined_classifier = CombinedClassifier(img_feature_dim=img_feature_dim, text_feature_dim=768, num_classes=2).to(device)\n","\n","# The rest of your training loop remains the same.\n","\n","\n","# Define optimizer and criterion\n","optimizer = torch.optim.AdamW(list(densenet121.parameters()) + list(bert_model.parameters()) + list(combined_classifier.parameters()), lr=1e-5)\n","criterion = nn.CrossEntropyLoss()\n","\n","# Set number of epochs and other parameters\n","num_epochs = 35\n","max_seq_length = 80\n","\n","train_losses = []\n","train_accuracies = []\n","val_losses = []\n","val_accuracies = []\n","\n","start_time = time.time()\n","\n","# Training loop\n","for epoch in range(num_epochs):\n","    densenet121.train()\n","    bert_model.train()\n","    combined_classifier.train()\n","    \n","    running_train_loss = 0.0\n","    correct_train = 0\n","    total_train = 0\n","\n","    for images, texts, labels in tqdm(train_loader, desc=f'Epoch {epoch + 1}/{num_epochs}', leave=False):\n","        # Move tensors to the device\n","        images = images.to(device)\n","        labels = labels.to(device)\n","        \n","        # Extract image features using MobileNetV2\n","        with torch.no_grad():\n","            img_feats = densenet121(images)\n","        \n","        # Reshape img_feats\n","        img_feats = img_feats.view(img_feats.size(0), -1)\n","\n","        # Convert texts to tensors and pad to a fixed sequence length\n","        texts = [bert_tokenizer(text, padding='max_length', truncation=True, max_length=max_seq_length, return_tensors='pt') for text in texts]\n","        input_ids = torch.stack([text['input_ids'].squeeze(0) for text in texts], dim=0).to(device)\n","        attention_mask = torch.stack([text['attention_mask'].squeeze(0) for text in texts], dim=0).to(device)\n","        \n","        optimizer.zero_grad()\n","\n","        outputs = bert_model(input_ids, attention_mask=attention_mask)\n","        text_feats = outputs.last_hidden_state[:, 0, :]\n","\n","        combined_logits = combined_classifier(img_feats, text_feats)\n","\n","        # Ensure the labels tensor is flattened to match the combined_logits batch size\n","        labels = labels.view(-1)\n","\n","        loss = criterion(combined_logits, labels)\n","\n","        loss.backward()\n","        optimizer.step()\n","\n","        running_train_loss += loss.item()\n","        _, predicted = combined_logits.max(1)\n","        total_train += labels.size(0)\n","        correct_train += predicted.eq(labels).sum().item()\n","\n","    epoch_train_loss = running_train_loss / len(train_loader)\n","    epoch_train_accuracy = correct_train / total_train\n","\n","    train_losses.append(epoch_train_loss)\n","    train_accuracies.append(epoch_train_accuracy)\n","    \n","    # Validation loop\n","    densenet121.eval()\n","    bert_model.eval()\n","    combined_classifier.eval()\n","\n","    running_val_loss = 0.0\n","    correct_val = 0\n","    total_val = 0\n","\n","    with torch.no_grad():\n","        for val_images, val_texts, val_labels in val_loader:\n","            val_images = val_images.to(device)\n","            val_labels = val_labels.to(device)\n","            \n","            val_img_feats = densenet121(val_images)\n","            val_img_feats = val_img_feats.view(val_img_feats.size(0), -1)\n","\n","            val_texts = [bert_tokenizer(text, padding='max_length', truncation=True, max_length=max_seq_length, return_tensors='pt') for text in val_texts]\n","            val_input_ids = torch.stack([text['input_ids'].squeeze(0) for text in val_texts], dim=0).to(device)\n","            val_attention_mask = torch.stack([text['attention_mask'].squeeze(0) for text in val_texts], dim=0).to(device)\n","\n","            val_outputs = bert_model(val_input_ids, attention_mask=val_attention_mask)\n","            val_text_feats = val_outputs.last_hidden_state[:, 0, :]\n","\n","            val_combined_logits = combined_classifier(val_img_feats, val_text_feats)\n","\n","            # Ensure the val_labels tensor is flattened to match the val_combined_logits batch size\n","            val_labels = val_labels.view(-1)\n","\n","            val_loss = criterion(val_combined_logits, val_labels)\n","\n","            running_val_loss += val_loss.item()\n","            _, val_predicted = val_combined_logits.max(1)\n","            \n","            total_val += val_labels.size(0)\n","            correct_val += val_predicted.eq(val_labels).sum().item()\n","\n","    epoch_val_loss = running_val_loss / len(val_loader)\n","    epoch_val_accuracy = correct_val / total_val\n","\n","    val_losses.append(epoch_val_loss)\n","    val_accuracies.append(epoch_val_accuracy)\n","\n","    print(f\"Epoch [{epoch + 1}/{num_epochs}] - \"\n","          f\"Train Loss: {epoch_train_loss:.4f}, Train Acc: {epoch_train_accuracy:.4f}, \"\n","          f\"Val Loss: {epoch_val_loss:.4f}, Val Acc: {epoch_val_accuracy:.4f}\")\n","\n","end_time = time.time()\n","execution_time = end_time - start_time\n","print(f\"Total execution time: {execution_time:.2f} seconds\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-18T07:15:33.101793Z","iopub.status.busy":"2024-07-18T07:15:33.100865Z","iopub.status.idle":"2024-07-18T07:16:05.675636Z","shell.execute_reply":"2024-07-18T07:16:05.674388Z","shell.execute_reply.started":"2024-07-18T07:15:33.101756Z"},"trusted":true},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix\n","\n","# Assuming test_loader contains your test data loader\n","\n","combined_classifier.eval()  # Set the model to evaluation mode\n","\n","test_losses = []\n","test_accuracies = []\n","predictions = []\n","true_labels = []\n","#test_loader \n","with torch.no_grad():\n","    for images, texts, labels in tqdm(test_loader, desc='Testing', leave=False):\n","        images = images.to(device)\n","        labels = labels.to(device)\n","\n","        texts = [bert_tokenizer(text, padding='max_length', truncation=True, max_length=max_seq_length, return_tensors='pt') for text in texts]\n","        input_ids = torch.stack([text['input_ids'].squeeze(0) for text in texts], dim=0).to(device)\n","        attention_mask = torch.stack([text['attention_mask'].squeeze(0) for text in texts], dim=0).to(device)\n","\n","        img_feats = densenet121(images)\n","        img_feats = img_feats.view(img_feats.size(0), -1)\n","\n","        outputs = bert_model(input_ids, attention_mask=attention_mask)\n","        text_feats = outputs.last_hidden_state[:, 0, :]\n","\n","        logits = combined_classifier(img_feats, text_feats)\n","\n","        # Calculate loss if needed\n","        test_loss = criterion(logits, labels)\n","        test_losses.append(test_loss.item())\n","\n","        # Calculate accuracy\n","        _, predicted = logits.max(1)\n","        predictions.extend(predicted.cpu().numpy())\n","        true_labels.extend(labels.cpu().numpy())\n","        correct = predicted.eq(labels).sum().item()\n","        total = labels.size(0)\n","        test_accuracy = correct / total\n","        test_accuracies.append(test_accuracy)\n","\n","# Calculate overall test metrics\n","average_test_loss = sum(test_losses) / len(test_losses)\n","test_accuracy = accuracy_score(true_labels, predictions)\n","precision, recall, f1_score, _ = precision_recall_fscore_support(true_labels, predictions, average='weighted')\n","conf_matrix = confusion_matrix(true_labels, predictions)\n","\n","# Print and plot results\n","print(f\"Test Loss: {average_test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n","print(f\"Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1_score:.4f}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-07-18T07:04:44.679717Z","iopub.status.idle":"2024-07-18T07:04:44.680056Z","shell.execute_reply":"2024-07-18T07:04:44.679901Z","shell.execute_reply.started":"2024-07-18T07:04:44.679887Z"},"trusted":true},"outputs":[],"source":["conf_matrix"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-07-18T07:04:44.681263Z","iopub.status.idle":"2024-07-18T07:04:44.681774Z","shell.execute_reply":"2024-07-18T07:04:44.681540Z","shell.execute_reply.started":"2024-07-18T07:04:44.681520Z"},"trusted":true},"outputs":[],"source":["# Plot confusion matrix\n","plt.figure(figsize=(8, 6))\n","# Class names according to the label encoding mapping\n","class_names = ['Real', 'Fake']\n","sns.heatmap(conf_matrix, annot=True, cmap='Blues', fmt='d',xticklabels=class_names, yticklabels=class_names)\n","plt.xlabel('Predicted labels')\n","plt.ylabel('True labels')\n","plt.title('Confusion Matrix')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":5344715,"sourceId":8881204,"sourceType":"datasetVersion"},{"datasetId":5316483,"sourceId":8973996,"sourceType":"datasetVersion"}],"dockerImageVersionId":30733,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
