{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-07-25T13:08:04.139046Z","iopub.status.busy":"2024-07-25T13:08:04.137996Z","iopub.status.idle":"2024-07-25T13:08:07.783550Z","shell.execute_reply":"2024-07-25T13:08:07.782565Z","shell.execute_reply.started":"2024-07-25T13:08:04.139011Z"},"trusted":true},"outputs":[],"source":["import pandas as pd\n","import os\n","from sklearn.preprocessing import LabelEncoder\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision import transforms\n","from PIL import Image\n","from transformers import BertTokenizer, BertModel, AdamW\n","from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix\n","import torchvision.models as models\n","from torch import nn\n","import time \n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","from tqdm import tqdm\n","\n","# Paths to the CSV files and image directories\n","csv_paths = {\n","    'train': '/kaggle/input/fake-news/Train.csv',\n","    'test': '/kaggle/input/fake-news/Test.csv',\n","    'validation': '/kaggle/input/fake-news/Val.csv'\n","}\n","\n","image_dirs = {\n","    'train': '/kaggle/input/fake-news/train',\n","    'test': '/kaggle/input/fake-news/test',\n","    'validation': '/kaggle/input/fake-news/validation'\n","}\n","output_dir = '/kaggle/working/'  # Output directory to save the CSV files\n","\n","# Function to check for matching Meme_ID and image files, and add image paths\n","def check_matches(csv_path, image_dir):\n","    # Read the CSV file into a DataFrame\n","    df = pd.read_csv(csv_path)\n","    \n","    # List all files in the image directory\n","    image_files = os.listdir(image_dir)\n","    \n","    # Create a dictionary to map image filenames (without extensions) to their full paths\n","    image_names = {os.path.splitext(image_file)[0]: os.path.join(image_dir, image_file) for image_file in image_files}\n","    \n","    # Add an Image_Path column to the DataFrame\n","    df['Image_Path'] = df['image_id'].apply(lambda x: image_names.get(x, None))\n","    \n","    return df\n","\n","# Function to encode Intent_Taxonomy classes into labels\n","def encode_labels(df):\n","    label_encoder = LabelEncoder()\n","    df['label'] = label_encoder.fit_transform(df['label'])\n","    return df, label_encoder.classes_\n","\n","# Check matches for each set (Train, Test, Validation)\n","for key in csv_paths:\n","    matched_df = check_matches(csv_paths[key], image_dirs[key])\n","    \n","    matches_output_path = os.path.join(output_dir, f'{key}_matches.csv')\n","    \n","    # Save the processed dataframe to CSV\n","    matched_df.to_csv(matches_output_path, index=False)\n","    \n","    print(f\"{key} set:\")\n","    print(f\"Matched image_ids with image paths and labels saved to {matches_output_path}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-25T13:08:11.480950Z","iopub.status.busy":"2024-07-25T13:08:11.480559Z","iopub.status.idle":"2024-07-25T13:08:11.735595Z","shell.execute_reply":"2024-07-25T13:08:11.734533Z","shell.execute_reply.started":"2024-07-25T13:08:11.480920Z"},"trusted":true},"outputs":[],"source":["train_df = pd.read_csv('/kaggle/working/train_matches.csv')\n","train_df.head(10)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-25T13:08:13.175106Z","iopub.status.busy":"2024-07-25T13:08:13.174429Z","iopub.status.idle":"2024-07-25T13:08:13.222445Z","shell.execute_reply":"2024-07-25T13:08:13.221483Z","shell.execute_reply.started":"2024-07-25T13:08:13.175075Z"},"trusted":true},"outputs":[],"source":["test_df = pd.read_csv('/kaggle/working/test_matches.csv')\n","test_df.head(10)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-25T13:08:13.846124Z","iopub.status.busy":"2024-07-25T13:08:13.845761Z","iopub.status.idle":"2024-07-25T13:08:13.895339Z","shell.execute_reply":"2024-07-25T13:08:13.894394Z","shell.execute_reply.started":"2024-07-25T13:08:13.846096Z"},"trusted":true},"outputs":[],"source":["validation_df = pd.read_csv('/kaggle/working/validation_matches.csv')\n","validation_df.head(10)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-25T13:08:14.708807Z","iopub.status.busy":"2024-07-25T13:08:14.708421Z","iopub.status.idle":"2024-07-25T13:08:14.718680Z","shell.execute_reply":"2024-07-25T13:08:14.717681Z","shell.execute_reply.started":"2024-07-25T13:08:14.708778Z"},"trusted":true},"outputs":[],"source":["# Define your transformations using transforms.Compose\n","transform = transforms.Compose([\n","    transforms.Resize(224),\n","    transforms.CenterCrop(224),  # Crop the center to 224x224\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n","])\n","\n","class MyMultimodalDataset(Dataset):\n","    def __init__(self, image_paths, description, label, transform=None):\n","        self.image_paths = image_paths\n","        self.description = description\n","        self.label = label\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.image_paths)\n","\n","    def __getitem__(self, idx):\n","        img_path = self.image_paths[idx]\n","        text = self.description[idx]\n","        label = self.label[idx]\n","        #print(img_path)\n","        \n","        # Ensure img_path is a string\n","        if not isinstance(img_path, str):\n","            raise ValueError(f\"Invalid image path at index {idx}: {img_path}\")\n","        # Load and preprocess image\n","        image = Image.open(img_path).convert('RGB')\n","        if self.transform:\n","            image = self.transform(image)\n","        \n","        return image, text, label"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-25T13:08:25.664812Z","iopub.status.busy":"2024-07-25T13:08:25.664413Z","iopub.status.idle":"2024-07-25T13:08:25.672149Z","shell.execute_reply":"2024-07-25T13:08:25.671070Z","shell.execute_reply.started":"2024-07-25T13:08:25.664782Z"},"trusted":true},"outputs":[],"source":["# Assuming you have lists or arrays of image paths, captions, and encoded labels:\n","train_dataset = MyMultimodalDataset(train_df['Image_Path'], train_df['description'], train_df['label'], transform=transform)\n","val_dataset = MyMultimodalDataset(validation_df['Image_Path'],validation_df['description'], validation_df['label'], transform=transform)\n","test_dataset = MyMultimodalDataset(test_df['Image_Path'],test_df['description'], test_df['label'], transform=transform)\n","\n","# Define data loaders\n","train_loader = DataLoader(train_dataset, batch_size=10, shuffle=True)\n","val_loader = DataLoader(val_dataset, batch_size=10, shuffle=False)\n","test_loader = DataLoader(test_dataset, batch_size=10, shuffle=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-25T14:27:31.947899Z","iopub.status.busy":"2024-07-25T14:27:31.947006Z","iopub.status.idle":"2024-07-25T14:27:31.952858Z","shell.execute_reply":"2024-07-25T14:27:31.951863Z","shell.execute_reply.started":"2024-07-25T14:27:31.947865Z"},"trusted":true},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from transformers import BertModel, BertTokenizer,AdamW\n","from tqdm import tqdm\n","import torchvision.models as models\n","import time\n","from torchvision.models import mobilenet_v2"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-25T14:27:46.494439Z","iopub.status.busy":"2024-07-25T14:27:46.494075Z","iopub.status.idle":"2024-07-25T14:27:46.499837Z","shell.execute_reply":"2024-07-25T14:27:46.498940Z","shell.execute_reply.started":"2024-07-25T14:27:46.494413Z"},"trusted":true},"outputs":[],"source":["# Check if GPU is available\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print('Using device:', device)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-25T14:27:47.709246Z","iopub.status.busy":"2024-07-25T14:27:47.708613Z","iopub.status.idle":"2024-07-25T14:27:48.065642Z","shell.execute_reply":"2024-07-25T14:27:48.064727Z","shell.execute_reply.started":"2024-07-25T14:27:47.709210Z"},"trusted":true},"outputs":[],"source":["# Initialize mobilenet_v2_model with IMAGENET1K_V1 weights\n","mobilenet_v2 = models.mobilenet_v2(weights='IMAGENET1K_V1', progress=True)\n","mobilenet_v2 = torch.nn.Sequential(*(list(mobilenet_v2.children())[:-1]))  # Remove the classification layer"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-25T14:45:22.823388Z","iopub.status.busy":"2024-07-25T14:45:22.822632Z","iopub.status.idle":"2024-07-25T14:45:23.818140Z","shell.execute_reply":"2024-07-25T14:45:23.817130Z","shell.execute_reply.started":"2024-07-25T14:45:22.823343Z"},"trusted":true},"outputs":[],"source":["# Initialize BERT tokenizer and model\n","bert_tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n","bert_model = BertModel.from_pretrained(\"bert-base-multilingual-cased\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-25T14:45:24.127886Z","iopub.status.busy":"2024-07-25T14:45:24.127508Z","iopub.status.idle":"2024-07-25T14:45:24.142226Z","shell.execute_reply":"2024-07-25T14:45:24.141353Z","shell.execute_reply.started":"2024-07-25T14:45:24.127856Z"},"trusted":true},"outputs":[],"source":["mobilenet_v2.to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-25T14:45:24.543618Z","iopub.status.busy":"2024-07-25T14:45:24.543238Z","iopub.status.idle":"2024-07-25T14:45:24.740800Z","shell.execute_reply":"2024-07-25T14:45:24.739775Z","shell.execute_reply.started":"2024-07-25T14:45:24.543588Z"},"trusted":true},"outputs":[],"source":["bert_model.to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-25T14:45:25.567060Z","iopub.status.busy":"2024-07-25T14:45:25.566374Z","iopub.status.idle":"2024-07-25T14:45:25.571855Z","shell.execute_reply":"2024-07-25T14:45:25.570715Z","shell.execute_reply.started":"2024-07-25T14:45:25.567025Z"},"trusted":true},"outputs":[],"source":["import torch\n","import time\n","from torch.optim import AdamW\n","from torchvision import transforms\n","from PIL import Image\n","from tqdm import tqdm\n","import torch.nn as nn"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-25T14:29:22.984560Z","iopub.status.busy":"2024-07-25T14:29:22.983883Z","iopub.status.idle":"2024-07-25T14:29:22.993436Z","shell.execute_reply":"2024-07-25T14:29:22.992405Z","shell.execute_reply.started":"2024-07-25T14:29:22.984528Z"},"trusted":true},"outputs":[],"source":["# Define optimizer and loss function\n","optimizer = AdamW(list(mobilenet_v2.parameters()) + list(bert_model.parameters()), lr=2e-5)\n","criterion = torch.nn.CrossEntropyLoss()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-25T14:29:35.226232Z","iopub.status.busy":"2024-07-25T14:29:35.225263Z","iopub.status.idle":"2024-07-25T14:34:48.423497Z","shell.execute_reply":"2024-07-25T14:34:48.422377Z","shell.execute_reply.started":"2024-07-25T14:29:35.226198Z"},"trusted":true},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from tqdm import tqdm\n","\n","class CombinedClassifier(nn.Module):\n","    def __init__(self, img_feature_dim, text_feature_dim, num_classes):\n","        super(CombinedClassifier, self).__init__()\n","        self.shared_fc = nn.Sequential(\n","            nn.Linear(img_feature_dim + text_feature_dim, 1024),\n","            nn.ReLU(),\n","            nn.Dropout(0.5),\n","            nn.Linear(1024, 512),\n","            nn.ReLU(),\n","            nn.Dropout(0.5),\n","            nn.Linear(512, num_classes)\n","        )\n","\n","    def forward(self, img_features, text_features):\n","        fused_features = torch.cat((img_features, text_features), dim=-1)  # Concatenate along the feature dimension\n","        combined_logits = self.shared_fc(fused_features)\n","        return combined_logits\n","\n","\n","# Example usage assuming img_feature_dim is correctly set\n","combined_classifier = CombinedClassifier(img_feature_dim=62720, text_feature_dim=768, num_classes=2).to(device)\n","\n","# Define optimizer and criterion\n","optimizer = optim.AdamW(list(mobilenet_v2.parameters()) + list(bert_model.parameters()) + list(combined_classifier.parameters()), lr=1e-5)\n","criterion = nn.CrossEntropyLoss()\n","\n","# Set number of epochs and other parameters\n","num_epochs = 35\n","max_seq_length = 80\n","\n","train_losses = []\n","train_accuracies = []\n","val_losses = []\n","val_accuracies = []\n","\n","start_time = time.time()\n","\n","# Training loop\n","for epoch in range(num_epochs):\n","    mobilenet_v2.train()\n","    bert_model.train()\n","    combined_classifier.train()\n","    \n","    running_train_loss = 0.0\n","    correct_train = 0\n","    total_train = 0\n","\n","    for images, texts, labels in tqdm(train_loader, desc=f'Epoch {epoch + 1}/{num_epochs}', leave=False):\n","        # Move tensors to the device\n","        images = images.to(device)\n","        labels = labels.to(device)\n","\n","        # Extract image features using mobilenet_v2\n","        with torch.no_grad():\n","            img_feats = mobilenet_v2(images)\n","        \n","        # Reshape img_feats\n","        img_feats = img_feats.view(img_feats.size(0), -1)\n","\n","        # Convert texts to tensors and pad to a fixed sequence length\n","        texts = [bert_tokenizer(text, padding='max_length', truncation=True, max_length=max_seq_length, return_tensors='pt') for text in texts]\n","        input_ids = torch.stack([text['input_ids'].squeeze(0) for text in texts], dim=0).to(device)\n","        attention_mask = torch.stack([text['attention_mask'].squeeze(0) for text in texts], dim=0).to(device)\n","\n","        optimizer.zero_grad()\n","\n","        outputs = bert_model(input_ids, attention_mask=attention_mask)\n","        text_feats = outputs.last_hidden_state[:, 0, :]\n","\n","        # Early fusion: concatenate raw image and text features\n","        combined_logits = combined_classifier(img_feats, text_feats)\n","\n","        # Ensure the labels tensor is flattened to match the combined_logits batch size\n","        labels = labels.view(-1)\n","\n","        loss = criterion(combined_logits, labels)\n","\n","        loss.backward()\n","        optimizer.step()\n","\n","        running_train_loss += loss.item()\n","        _, predicted = combined_logits.max(1)\n","        total_train += labels.size(0)\n","        correct_train += predicted.eq(labels).sum().item()\n","\n","    epoch_train_loss = running_train_loss / len(train_loader)\n","    epoch_train_accuracy = correct_train / total_train\n","\n","    train_losses.append(epoch_train_loss)\n","    train_accuracies.append(epoch_train_accuracy)\n","    \n","    # Validation loop\n","    mobilenet_v2.eval()\n","    bert_model.eval()\n","    combined_classifier.eval()\n","\n","    running_val_loss = 0.0\n","    correct_val = 0\n","    total_val = 0\n","\n","    with torch.no_grad():\n","        for val_images, val_texts, val_labels in val_loader:\n","            val_images = val_images.to(device)\n","            val_labels = val_labels.to(device)\n","\n","            val_img_feats = mobilenet_v2(val_images)\n","            val_img_feats = val_img_feats.view(val_img_feats.size(0), -1)\n","\n","            val_texts = [bert_tokenizer(text, padding='max_length', truncation=True, max_length=max_seq_length, return_tensors='pt') for text in val_texts]\n","            val_input_ids = torch.stack([text['input_ids'].squeeze(0) for text in val_texts], dim=0).to(device)\n","            val_attention_mask = torch.stack([text['attention_mask'].squeeze(0) for text in val_texts], dim=0).to(device)\n","\n","            val_outputs = bert_model(val_input_ids, attention_mask=val_attention_mask)\n","            val_text_feats = val_outputs.last_hidden_state[:, 0, :]\n","\n","            # Early fusion: concatenate raw image and text features\n","            val_combined_logits = combined_classifier(val_img_feats, val_text_feats)\n","\n","            # Ensure the val_labels tensor is flattened to match the val_combined_logits batch size\n","            val_labels = val_labels.view(-1)\n","\n","            val_loss = criterion(val_combined_logits, val_labels)\n","\n","            running_val_loss += val_loss.item()\n","            _, val_predicted = val_combined_logits.max(1)\n","            total_val += val_labels.size(0)\n","            correct_val += val_predicted.eq(val_labels).sum().item()\n","\n","    epoch_val_loss = running_val_loss / len(val_loader)\n","    epoch_val_accuracy = correct_val / total_val\n","\n","    val_losses.append(epoch_val_loss)\n","    val_accuracies.append(epoch_val_accuracy)\n","\n","    print(f\"Epoch [{epoch + 1}/{num_epochs}] - \"\n","          f\"Train Loss: {epoch_train_loss:.4f}, Train Acc: {epoch_train_accuracy:.4f}, \"\n","          f\"Val Loss: {epoch_val_loss:.4f}, Val Acc: {epoch_val_accuracy:.4f}\")\n","\n","end_time = time.time()\n","execution_time = end_time - start_time\n","print(f\"Total execution time: {execution_time:.2f} seconds\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-25T14:37:10.619183Z","iopub.status.busy":"2024-07-25T14:37:10.618773Z","iopub.status.idle":"2024-07-25T14:37:27.841903Z","shell.execute_reply":"2024-07-25T14:37:27.840885Z","shell.execute_reply.started":"2024-07-25T14:37:10.619153Z"},"trusted":true},"outputs":[],"source":["import numpy as np\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n","predicted_labels = []\n","true_labels = []\n","\n","# Set the models to evaluation mode\n","mobilenet_v2.eval()\n","bert_model.eval()\n","combined_classifier.eval()\n","\n","with torch.no_grad():\n","    for test_images, test_texts, test_labels in tqdm(test_loader, desc='Testing', leave=False):\n","        # Move tensors to the device\n","        test_images = test_images.to(device)\n","        test_labels = test_labels.to(device)\n","\n","        # Extract image features using mobilenet_v2\n","        test_img_feats = mobilenet_v2(test_images)\n","        test_img_feats = test_img_feats.view(test_img_feats.size(0), -1)\n","\n","        # Convert texts to tensors and pad to a fixed sequence length\n","        test_texts = [bert_tokenizer(text, padding='max_length', truncation=True, max_length=max_seq_length, return_tensors='pt') for text in test_texts]\n","        test_input_ids = torch.stack([text['input_ids'].squeeze(0) for text in test_texts], dim=0).to(device)\n","        test_attention_mask = torch.stack([text['attention_mask'].squeeze(0) for text in test_texts], dim=0).to(device)\n","\n","        test_outputs = bert_model(test_input_ids, attention_mask=test_attention_mask)\n","        test_text_feats = test_outputs.last_hidden_state[:, 0, :]\n","\n","        # Early fusion: concatenate raw image and text features\n","        test_combined_logits = combined_classifier(test_img_feats, test_text_feats)\n","\n","        # Get the predicted labels\n","        _, test_predicted = test_combined_logits.max(1)\n","\n","        # Store predicted and true labels\n","        predicted_labels.extend(test_predicted.cpu().numpy())\n","        true_labels.extend(test_labels.cpu().numpy())\n","\n","# Convert lists to numpy arrays for further analysis\n","predicted_labels = np.array(predicted_labels)\n","true_labels = np.array(true_labels)\n","\n","# Print accuracy\n","# Calculate metrics\n","accuracy = accuracy_score(true_labels, predicted_labels)\n","precision = precision_score(true_labels, predicted_labels, average='weighted')\n","recall = recall_score(true_labels, predicted_labels, average='weighted')\n","f1 = f1_score(true_labels, predicted_labels, average='weighted')\n","conf_matrix = confusion_matrix(true_labels, predicted_labels)\n","\n","# Print metrics\n","print(f\"Test Accuracy: {accuracy:.4f}\")\n","print(f\"Test Precision: {precision:.4f}\")\n","print(f\"Test Recall: {recall:.4f}\")\n","print(f\"Test F1 Score: {f1:.4f}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-25T14:37:27.844101Z","iopub.status.busy":"2024-07-25T14:37:27.843815Z","iopub.status.idle":"2024-07-25T14:37:27.850401Z","shell.execute_reply":"2024-07-25T14:37:27.849241Z","shell.execute_reply.started":"2024-07-25T14:37:27.844075Z"},"trusted":true},"outputs":[],"source":["conf_matrix"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-25T14:37:27.851830Z","iopub.status.busy":"2024-07-25T14:37:27.851546Z","iopub.status.idle":"2024-07-25T14:37:28.087287Z","shell.execute_reply":"2024-07-25T14:37:28.086351Z","shell.execute_reply.started":"2024-07-25T14:37:27.851805Z"},"trusted":true},"outputs":[],"source":["# Plot confusion matrix\n","plt.figure(figsize=(8, 6))\n","# Class names according to the label encoding mapping\n","class_names = ['Real', 'Fake']\n","sns.heatmap(conf_matrix, annot=True, cmap='Blues', fmt='d',xticklabels=class_names, yticklabels=class_names)\n","plt.xlabel('Predicted labels')\n","plt.ylabel('True labels')\n","plt.title('Confusion Matrix')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":5344715,"sourceId":8881204,"sourceType":"datasetVersion"},{"datasetId":5316483,"sourceId":8973996,"sourceType":"datasetVersion"}],"dockerImageVersionId":30747,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
